{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSCI5953VeenaAmbalavanan.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwbYBGrRVHP7"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from numpy import expand_dims\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy import asarray\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "import random\n",
        "import glob\n",
        "import os\n",
        "import itertools\n",
        "import math\n",
        "import copy\n",
        "from PIL import Image\n",
        "from skimage.transform import resize\n",
        "from contextlib import redirect_stdout \n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Conv2D, Conv2DTranspose, Dropout\n",
        "from keras.layers import Activation, LeakyReLU, BatchNormalization, Flatten, Reshape\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import plot_model, np_utils\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from keras.preprocessing import image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Qlbqb1nVHP-"
      },
      "source": [
        "# # For integrating drive with colab\n",
        "# from google.colab import files\n",
        "\n",
        "# #  Google Drive Authentication\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeu5OO1fVHQA"
      },
      "source": [
        "path = '/content/drive/My Drive/src/'\n",
        "\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "os.chdir(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NASz1ybXVHQF"
      },
      "source": [
        "## Loading and Preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4mOIR-X3iqA",
        "outputId": "28ae137f-6c97-4749-9e11-c2983d28b84d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj3IFIIl3qZ1"
      },
      "source": [
        "\n",
        "!unzip -uq \"/content/gdrive/MyDrive/Places365val\" -d \"/content/gdrive/MyDrive/Places365val\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOE0kYY0zzWh"
      },
      "source": [
        "# import cv2\n",
        "# import os\n",
        "\n",
        "# def load_images_from_folder(folder):\n",
        "#     images = []\n",
        "#     for filename in os.listdir(folder):\n",
        "#         img = cv2.imread(os.path.join(folder,filename))\n",
        "#         if img is not None:\n",
        "#             images.append(img)\n",
        "#     return images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJJLIv5qz3zH"
      },
      "source": [
        "# load_images_from_folder(\"/content/drive/My Drive/src/raw_dataset\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40_qfe0azwKs"
      },
      "source": [
        "#read the images and compress to 128*128*3\n",
        "path = '/content/gdrive/MyDrive/src/raw_dataset/*.jpg'  \n",
        "images = [cv2.imread(file) for file in glob.glob(path)]\n",
        "imagesnp = np.asarray(images) \n",
        "# print(imagesnp.size)\n",
        "\n",
        "resize_images = []\n",
        "for image in imagesnp:\n",
        "    compressedimage = resize(image, (128, 128, 3), anti_aliasing=True)\n",
        "    resize_images.append(compressedimage)\n",
        "resize_images = np.asarray(resize_images)\n",
        "# print(imagesnp.size)\n",
        "\n",
        "# for filename in os.listdir(folder):\n",
        "#         img = cv2.imread(os.path.join(folder,filename))\n",
        "#         if img is not None:\n",
        "#             images.append(img)\n",
        "\n",
        "# Normalise the dataset\n",
        "X_train = resize_images/255.0 \n",
        "X_temp  = copy.deepcopy(X_train)\n",
        "\n",
        "\n",
        "# Creating mask\n",
        "total = X_temp.shape[0] #4000\n",
        "mask_shape  = (total,128,128,1)\n",
        "mask = np.zeros(mask_shape)\n",
        "padding_width = 32\n",
        "mask[:, :, :padding_width, :] = 1.0 # left portion\n",
        "mask[:, :,-padding_width:, :] = 1.0 # right portion\n",
        "pix_avg = np.mean(X_temp, axis=(1,2,3))\n",
        "X_temp[:, :, :padding_width, :] = pix_avg[:, np.newaxis, np.newaxis, np.newaxis]\n",
        "X_temp[:, :,-padding_width:, :] = pix_avg[:, np.newaxis, np.newaxis, np.newaxis]\n",
        "X_mask = np.concatenate((X_temp, mask), axis=3)\n",
        "np.savez('places_dataset.npz', X_train=X_train, X_mask=X_mask) \n",
        "\n",
        "\n",
        "#Load the numpy dataset\n",
        "data = np.load('places_dataset.npz')\n",
        "X_train = np.load['X_train']\n",
        "X_mask  = data['X_mask']\n",
        "\n",
        "np.save('x_mask', X_mask)\n",
        "np.save('x_train', X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYGa50fXzzfo"
      },
      "source": [
        "(X_temp.shape) # (4000, 128, 128, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al9dIGIyVHQK"
      },
      "source": [
        "# Define Generator model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cXvDdAtVHQR"
      },
      "source": [
        "def build_generator():\n",
        "    Gen = Sequential(name='Generator')\n",
        "\n",
        "    #intialiser - generates tensor with normal distribution\n",
        "    kernalinit = tf.keras.initializers.RandomNormal(stddev=0.01, seed= None)\n",
        "\n",
        "    #Encoder block\n",
        "    Gen.add(Conv2D(64, (5,5), strides=1, dilation_rate=(1,1), input_shape = (128,128,4),kernel_initializer=kernalinit, padding='same', activation='relu' ))\n",
        "    Gen.add(Conv2D(128, (3,3), strides=2, dilation_rate=(1,1), padding='same', activation='relu')) \n",
        "    Gen.add(Conv2D(256, (3,3), strides=1, dilation_rate=(1,1), padding='same', activation='relu'))\n",
        "    Gen.add(Conv2D(256, (3,3), strides=1, dilation_rate=(2,2), padding='same', activation='relu'))\n",
        "    Gen.add(BatchNormalization(momentum=0.9))\n",
        "    Gen.add(Conv2D(256, (3,3), strides=1, dilation_rate=(4,4), padding='same', activation='relu'))\n",
        "    Gen.add(BatchNormalization(momentum=0.9))\n",
        "    Gen.add(Conv2D(256, (3,3), strides=1, dilation_rate=(8,8), padding='same', activation='relu'))\n",
        "    Gen.add(BatchNormalization(momentum=0.9))\n",
        "    Gen.add(Conv2D(256, (3,3), strides=1, dilation_rate=(1,1), padding='same', activation='relu'))\n",
        "    Gen.add(BatchNormalization(momentum=0.9))\n",
        "\n",
        "    #Decoder block\n",
        "    Gen.add(Conv2DTranspose(128,(4,4),strides=2, padding='same', activation='relu')) \n",
        "    Gen.add(BatchNormalization(momentum=0.9)) \n",
        "    Gen.add(Conv2D(64, (3,3), strides=1, dilation_rate = (1,1), padding='same', activation='relu'))\n",
        "    Gen.add(BatchNormalization(momentum=0.9)) \n",
        "    Gen.add(Conv2D(3, (3,3), strides=1, activation='sigmoid', padding='same'))\n",
        "    return Gen\n",
        "\n",
        "# plot_model(build_generator())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5Vt_e2dVHQV"
      },
      "source": [
        "# Save the summary and architecture of the Generator model\n",
        "\n",
        "# with open('./drive/My Drive/Colab Notebooks/Generator.txt', 'w') as f:\n",
        "#     with redirect_stdout(f):\n",
        "#         build_generator().summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8ncNPKSp5Nz"
      },
      "source": [
        "# Define Discriminator model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlHiuqY_VHQL"
      },
      "source": [
        "def build_discriminator():\n",
        "    \n",
        "    Disc = Sequential(name='Discriminator')\n",
        "    \n",
        "    #intialiser - generates tensor with normal distribution\n",
        "    kernalinit = tf.keras.initializers.RandomNormal(stddev=0.01, seed= None)\n",
        "    # we use strided convolution in discriminators to repeatedly downsample the image\n",
        "    Disc.add(Conv2D(32, (5,5), strides=2,padding='same',input_shape=(128,128,3), kernel_initializer=kernalinit,  activation='relu'))\n",
        "    Disc.add(Conv2D(64, (5,5), strides=2, padding='same', activation='relu'))\n",
        "    Disc.add(Conv2D(64, (5,5), strides=2, padding='same', activation='relu'))\n",
        "    Disc.add(BatchNormalization(momentum=0.9))\n",
        "    Disc.add(Conv2D(64, (5,5), strides=2, padding='same', activation='relu'))\n",
        "    Disc.add(BatchNormalization(momentum=0.9))\n",
        "    Disc.add(Conv2D(64, (5,5), strides=2, padding='same', activation='relu'))\n",
        "    Disc.add(BatchNormalization(momentum=0.9))\n",
        "    Disc.add(Flatten())\n",
        "    Disc.add(Dropout(0.4))\n",
        "    Disc.add(Dense(512, activation='relu'))  \n",
        "    Disc.add(Dense(1,   activation='sigmoid'))  \n",
        "    return Disc\n",
        "\n",
        "# plot_model(build_discriminator())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSL2SLINVHQP"
      },
      "source": [
        "#Save the Summary and Architecture of discriminator model \n",
        "\n",
        "# with open('./drive/My Drive/Colab Notebooks/Discriminator.txt', 'w') as f:\n",
        "#     with redirect_stdout(f):\n",
        "#         build_discriminator(0.2,0.02).summary()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h4l6tNaqBk4"
      },
      "source": [
        "## Create iteration_count.txt and losses.txt files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWNSUa-wo6yk"
      },
      "source": [
        "if not os.path.exists('iteration_count.txt'):\n",
        "    with open(\"iteration_count.txt\", \"a\") as f:\n",
        "        f.write(\"Iteration_count : \\n\")\n",
        "        \n",
        "if not os.path.exists('losses.txt'):\n",
        "    with open(\"losses.txt\", \"a\") as f:\n",
        "        f.write(\"D_Real_Loss D_Fake_Loss D_Total_loss G_Loss\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH3f2vCHqLiZ"
      },
      "source": [
        "# Defining Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDiEBxq9VHQX"
      },
      "source": [
        "def G_loss_MSE(y_true, y_pred):\n",
        "    diff = y_pred - y_true\n",
        "    mse_loss = K.mean(K.square(diff))\n",
        "    return mse_loss\n",
        "\n",
        "def G_loss(y_true, y_pred):\n",
        "    mse_loss = G_loss_MSE(y_true, y_pred)\n",
        "    fake_loss = tf.log(tf.maximum(y_pred, K.epsilon()))\n",
        "    return mse_loss - 0.001*tf.reduce_mean(fake_loss)  \n",
        "\n",
        "def D_loss(y_true, y_pred):\n",
        "    real_prediction = tf.maximum(y_true, K.epsilon())\n",
        "    fake_prediction = tf.maximum(1.0 - y_pred, K.epsilon())\n",
        "    real_loss = tf.log(real_prediction)\n",
        "    fake_loss = tf.log(fake_prediction)\n",
        "    loss = -tf.reduce_mean(real_loss+fake_loss)\n",
        "    return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgumwDtSqUYt"
      },
      "source": [
        "## Build GAN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTNfye9bnw-3"
      },
      "source": [
        "def build_gan(g_learning_rate, d_learning_rate, g_beta_1, d_beta_1, flag = True):\n",
        "    \n",
        "    # discriminator\n",
        "    dis = build_discriminator()\n",
        "    D_optimizer = Adam(learning_rate=d_learning_rate, beta_1=d_beta_1)\n",
        "    dis.compile(optimizer = D_optimizer,loss = D_loss, metrics=['binary_accuracy'])\n",
        "    \n",
        "    # generator\n",
        "    gen = build_generator()\n",
        "\n",
        "    # gan\n",
        "    gan = Sequential([gen,dis])\n",
        "    gan_optimizer = Adam(learning_rate=g_learning_rate, beta_1= g_beta_1)\n",
        "    \n",
        "    if flag == True:\n",
        "        gan.compile(optimizer=gan_optimizer, loss=G_loss_MSE, metrics=['binary_accuracy'])\n",
        "    else:\n",
        "        gan.compile(optimizer=gan_optimizer, loss=G_loss, metrics=['binary_accuracy'])\n",
        "    \n",
        "    return gan, gen, dis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyLI7SLZVHQZ"
      },
      "source": [
        "#make the models trainable or untrainable based on need\n",
        "def trainable(gan_model):\n",
        "    for layer in gan_model.layers:\n",
        "        layer.trainable = True\n",
        "\n",
        "def non_trainable(gan_model):\n",
        "    for layer in gan_model.layers:\n",
        "        layer.trainable = False       \n",
        "        \n",
        "# if input is real data, set as np.ones and if fake, set as np.zeros\n",
        "def make_labels(batch_size, isReal):\n",
        "    if isReal==True:\n",
        "        return np.ones([batch_size, 1])\n",
        "    else:\n",
        "        return np.zeros([batch_size, 1])\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0DD4XZMrNpF"
      },
      "source": [
        "## Training GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnQGiwVIVHQf"
      },
      "source": [
        "def train_gan(g_learning_rate, d_learning_rate,g_beta_1,d_beta_1):\n",
        "\n",
        "    max_iters = 1000\n",
        "    gen_iters = 200\n",
        "    disc_iters = 20\n",
        "    batch_size = 64     \n",
        "    \n",
        "    # if old gan, generator and discriminator exist, load them, else build new\n",
        "    if os.path.exists('generator.h5'):\n",
        "        generator     = load_model('generator.h5')\n",
        "        discriminator = load_model('discriminator.h5')\n",
        "        gan           = load_model('gan.h5')\n",
        "    else:\n",
        "        gan, generator, discriminator = build_gan(g_learning_rate, g_beta_1, d_learning_rate, d_beta_1)\n",
        "    \n",
        "    # if iteration_count.txt exists, then get the prev_iter as model is trained before\n",
        "    prev_iter = 0\n",
        "    if os.path.exists('iteration_count.txt'):\n",
        "        prev_iter = sum(1 for line in open('iteration_count.txt'))-1\n",
        "    print('Previous iteration count :',prev_iter)\n",
        "    \n",
        "    # labels for real and fake images\n",
        "    y_train_real = make_labels(batch_size,True)\n",
        "    y_train_fake = make_labels(batch_size,False)\n",
        "    \n",
        "    for iters in range(prev_iter, max_iters):\n",
        "        print(\"Iteration :\",iters)\n",
        "        with open('iteration_count.txt', 'w') as f:\n",
        "            f.write(str(iters))\n",
        "        \n",
        "        # Train the generator\n",
        "        if iters < gen_iters: \n",
        "            discriminator.trainable = False \n",
        "            X_batch_masked = X_mask[np.random.choice(X_mask.shape[0], batch_size , replace = True), :] \n",
        "            gan.train_on_batch(X_batch_masked, y_train_real)\n",
        "    \n",
        "    \n",
        "        # Train the discriminator\n",
        "        elif iters < (gen_iters + disc_iters):\n",
        "            discriminator.trainable = True \n",
        "            X_batch_real   = X_train[np.random.choice(X_train.shape[0], batch_size , replace = True), :] \n",
        "            X_batch_masked = X_mask[np.random.choice(X_mask.shape[0], batch_size , replace = True), :] \n",
        "            X_batch_fake   = generator.predict_on_batch(X_batch_masked)\n",
        "\n",
        "            discriminator.train_on_batch(X_batch_real, y_train_real)\n",
        "            discriminator.train_on_batch(X_batch_fake, y_train_fake)\n",
        "        \n",
        "        #  Train both generator and discriminator adversarially\n",
        "        else:\n",
        "            if iters==(gen_iters + disc_iters):\n",
        "                gan, generator,_ = build_gan(g_learning_rate,d_learning_rate, g_beta_1, d_beta_1, flag = False)\n",
        "                gan.set_weights(gan.get_weights())\n",
        "                generator.set_weights(generator.get_weights())\n",
        "                \n",
        "            # Train discriminator\n",
        "            discriminator.trainable = True \n",
        "\n",
        "            X_batch_real   = X_train[np.random.choice(X_train.shape[0], batch_size , replace = True), :] \n",
        "            X_batch_masked = X_mask[np.random.choice(X_mask.shape[0], batch_size , replace = True), :] \n",
        "            X_batch_fake   = generator.predict_on_batch(X_batch_masked)\n",
        "\n",
        "            discriminator.train_on_batch(X_batch_real, y_train_real)\n",
        "            discriminator.train_on_batch(X_batch_fake, y_train_fake)\n",
        "\n",
        "            # Train generator\n",
        "            discriminator.trainable = False \n",
        "            gan.train_on_batch(X_batch_masked, y_train_real)\n",
        "        \n",
        "        \n",
        "        # Training Losses\n",
        "        if (iters + 1) % 100 == 0:\n",
        "            X_batch_masked = X_mask[np.random.choice(X_mask.shape[0], batch_size , replace = True), :]        \n",
        "\n",
        "            gan_images     = generator.predict_on_batch(X_batch_masked)\n",
        "            g_loss_batch   = gan.test_on_batch(X_batch_masked, y_train_real)\n",
        "\n",
        "            X_batch_real    = X_train[np.random.choice(X_train.shape[0], batch_size , replace = True), :] \n",
        "\n",
        "            d_loss_real  = discriminator.test_on_batch(X_batch_real, y_train_real)\n",
        "            d_loss_fake  = discriminator.test_on_batch(gan_images, y_train_fake)\n",
        "\n",
        "            with open(\"losses.txt\", \"a\") as f:\n",
        "                total = 0.5 * ( round(d_loss_real[0],12) + round(d_loss_fake[0],12) )\n",
        "                f.write( str(round(d_loss_real[0],12)) + ' ' + str(round(d_loss_fake[0],12)) +' ' + str(total) + ' ' + str( round(g_loss_batch[0],12) ) )\n",
        "                f.write('\\n')\n",
        "        \n",
        "\n",
        "        # Save models at regular intervals once gan starts training adversarially\n",
        "        if (iters >= (gen_iters + disc_iters)) and ((iters + 1) % 100 == 0):\n",
        "            if os.path.exists('generator.h5'):\n",
        "                os.remove('generator.h5')\n",
        "            generator.save('generator.h5')\n",
        "\n",
        "            if os.path.exists(\"discriminator.h5\"):\n",
        "                os.remove(\"discriminator.h5\")\n",
        "            discriminator.save('discriminator.h5')\n",
        "\n",
        "            if os.path.exists(\"gan.h5\"):\n",
        "                os.remove(\"gan.h5\")\n",
        "            gan.save('gan.h5')\n",
        "            \n",
        "    \n",
        "    return gan, generator, discriminator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRnMJizNrccf"
      },
      "source": [
        "gan, generator, discriminator = train_gan(g_learning_rate=0.0001,d_learning_rate=0.0001,g_beta_1=0.4,d_beta_1=0.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXRK5TLm0Po9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}